{
 "cells": [
  {
   "cell_type": "code",
   "id": "5fa65546-9057-4aa7-9a41-e2c6822e79b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:35.050487Z",
     "start_time": "2024-12-10T23:10:34.089262Z"
    }
   },
   "source": [
    "# import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import openpyxl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "b6a00907-e264-4b81-84b1-cda1f5d4523b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:35.056856Z",
     "start_time": "2024-12-10T23:10:35.053656Z"
    }
   },
   "source": [
    "class CustomOneHotEncoder:\n",
    "    def __init__(self):\n",
    "        self.categories_ = {}\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit the encoder by finding unique values for each column.\"\"\"\n",
    "        for col_idx in range(X.shape[1]):  # Iterate over columns\n",
    "            unique_vals = np.unique(X[:, col_idx])\n",
    "            self.categories_[col_idx] = {val: idx for idx, val in enumerate(unique_vals)}\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform the data into one-hot encoded format.\"\"\"\n",
    "        encoded_columns = []\n",
    "        for col_idx in range(X.shape[1]):  # Iterate over columns\n",
    "            unique_vals = self.categories_[col_idx]\n",
    "            encoded_col = np.zeros((X.shape[0], len(unique_vals)))\n",
    "            for row_idx, value in enumerate(X[:, col_idx]):\n",
    "                if value in unique_vals:\n",
    "                    encoded_col[row_idx, unique_vals[value]] = 1\n",
    "            encoded_columns.append(encoded_col)\n",
    "        return np.hstack(encoded_columns)\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit and transform the data in one step.\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "3cc64f0b-7aeb-4fc5-9081-51470f086174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:35.120166Z",
     "start_time": "2024-12-10T23:10:35.117597Z"
    }
   },
   "source": [
    "class StandardScaler:\n",
    "    def __init__(self):\n",
    "        self.means_ = None\n",
    "        self.stds_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Compute the mean and standard deviation for each feature.\"\"\"\n",
    "        self.means_ = np.mean(X, axis=0)\n",
    "        self.stds_ = np.std(X, axis=0)\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Standardize features by subtracting the mean and dividing by the standard deviation.\"\"\"\n",
    "        if self.means_ is None or self.stds_ is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "        return (X - self.means_) / self.stds_\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit to data, then transform it.\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def inverse_transform(self, X_scaled):\n",
    "        \"\"\"Revert the standardization (return to original scale).\"\"\"\n",
    "        if self.means_ is None or self.stds_ is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "        return (X_scaled * self.stds_) + self.means_\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "7964c583-aa78-44f4-8cc0-3116a4bbb7a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:35.169794Z",
     "start_time": "2024-12-10T23:10:35.164001Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def custom_train_test_split_old(X, y, test_size=0.2, random_state=None, stratify=None):\n",
    "    \"\"\"\n",
    "    Split arrays or DataFrame into random train and test subsets with optional stratification.\n",
    "\n",
    "    Parameters:\n",
    "        X (array-like or DataFrame): Features data.\n",
    "        y (array-like or Series): Target data.\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "        stratify (array-like or Series): Column for stratified splitting.\n",
    "\n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test: Split data with same type as input.\n",
    "    \"\"\"\n",
    "    is_X_dataframe = isinstance(X, pd.DataFrame)\n",
    "    is_y_series = isinstance(y, pd.Series)\n",
    "    \n",
    "    X = np.array(X) if not is_X_dataframe else X\n",
    "    y = np.array(y) if not is_y_series else y\n",
    "    \n",
    "    if stratify is not None:\n",
    "        stratify = np.array(stratify) if not isinstance(stratify, pd.Series) else stratify\n",
    "\n",
    "    # Random seed\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    # Stratified split\n",
    "    if stratify is not None:\n",
    "        unique_classes, class_indices = np.unique(stratify, return_inverse=True)\n",
    "        train_indices = []\n",
    "        test_indices = []\n",
    "        print(f\"unique_classes = {unique_classes}, class_indices = {class_indices}\")\n",
    "        # for class_idx, class_label in enumerate(unique_classes):\n",
    "        #     class_mask = (class_indices == class_idx)\n",
    "        #     class_indices = np.where(class_mask)[0]\n",
    "        #     np.random.shuffle(class_indices)\n",
    "\n",
    "        #     test_size_class = int(len(class_indices) * test_size)\n",
    "        #     test_indices.extend(class_indices[:test_size_class])\n",
    "        #     train_indices.extend(class_indices[test_size_class:])\n",
    "        for class_label in range(len(unique_classes)):  # Iterate over class indices\n",
    "            # Mask for rows belonging to the current class\n",
    "            class_mask = (class_indices == class_label)  # True for rows in this class\n",
    "            class_rows = np.where(class_mask)[0]  # Indices of rows in this class\n",
    "            \n",
    "            # Shuffle the indices for this class\n",
    "            np.random.shuffle(class_rows)\n",
    "            \n",
    "            # Calculate the number of test samples for this class\n",
    "            test_size_class = int(len(class_rows) * test_size)\n",
    "            \n",
    "            # Split into train and test\n",
    "            test_indices.extend(class_rows[:test_size_class])  # Add test rows\n",
    "            train_indices.extend(class_rows[test_size_class:])  # Add train rows\n",
    "        # print(f\"train_indices = {train_indices}\")\n",
    "    else:\n",
    "        indices = np.arange(len(X))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        test_size_count = int(len(X) * test_size)\n",
    "        test_indices = indices[:test_size_count]\n",
    "        train_indices = indices[test_size_count:]\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    # print(f\"y_train = \\n{y_train}\")\n",
    "    # print(f\"y_test = \\n{y_test}\")\n",
    "    # Return the data with the original type\n",
    "    if is_X_dataframe:\n",
    "        X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "        X_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "    print(y_train)\n",
    "    if is_y_series:\n",
    "        y_train = pd.Series(y_train, index=train_indices, name=y.name)\n",
    "        y_test = pd.Series(y_test, index=test_indices, name=y.name)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def custom_train_test_split(X, y, test_size=0.2, random_state=None, stratify=None):\n",
    "    \"\"\"\n",
    "    Split arrays or DataFrame into random train and test subsets with optional stratification.\n",
    "\n",
    "    Parameters:\n",
    "        X (DataFrame or array-like): Features data.\n",
    "        y (Series or array-like): Target data.\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "        stratify (array-like or Series): Column for stratified splitting.\n",
    "\n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test: Split data, retaining original data types.\n",
    "    \"\"\"\n",
    "    # Check if inputs are DataFrame/Series\n",
    "    is_X_dataframe = isinstance(X, pd.DataFrame)\n",
    "    is_y_series = isinstance(y, pd.Series)\n",
    "\n",
    "    # Reset DataFrame indices for consistency\n",
    "    if is_X_dataframe:\n",
    "        X_reset = X.reset_index(drop=True)\n",
    "    else:\n",
    "        X_reset = np.array(X)\n",
    "\n",
    "    if is_y_series:\n",
    "        y_reset = y.reset_index(drop=True)\n",
    "    else:\n",
    "        y_reset = np.array(y)\n",
    "\n",
    "    if stratify is not None and isinstance(stratify, pd.Series):\n",
    "        stratify = stratify.reset_index(drop=True)\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    # Stratified splitting\n",
    "    if stratify is not None:\n",
    "        unique_classes, class_indices = np.unique(stratify, return_inverse=True)\n",
    "        train_indices = []\n",
    "        test_indices = []\n",
    "\n",
    "        for class_label in range(len(unique_classes)):\n",
    "            class_mask = (class_indices == class_label)\n",
    "            class_rows = np.where(class_mask)[0]\n",
    "            np.random.shuffle(class_rows)\n",
    "\n",
    "            test_size_class = int(len(class_rows) * test_size)\n",
    "\n",
    "            test_indices.extend(class_rows[:test_size_class])\n",
    "            train_indices.extend(class_rows[test_size_class:])\n",
    "    else:\n",
    "        # Random split without stratification\n",
    "        indices = np.arange(len(X_reset))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        test_size_count = int(len(X_reset) * test_size)\n",
    "        test_indices = indices[:test_size_count]\n",
    "        train_indices = indices[test_size_count:]\n",
    "\n",
    "    # Use iloc to split DataFrame/Series\n",
    "    if is_X_dataframe:\n",
    "        X_train = X_reset.iloc[train_indices]\n",
    "        X_test = X_reset.iloc[test_indices]\n",
    "    else:\n",
    "        X_train, X_test = X_reset[train_indices], X_reset[test_indices]\n",
    "\n",
    "    if is_y_series:\n",
    "        y_train = y_reset.iloc[train_indices]\n",
    "        y_test = y_reset.iloc[test_indices]\n",
    "    else:\n",
    "        y_train, y_test = y_reset[train_indices], y_reset[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "58b456f2-7aed-4b4f-b213-101eff0af1e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:39.349192Z",
     "start_time": "2024-12-10T23:10:35.208537Z"
    }
   },
   "source": [
    "df = pd.read_excel('StockX-Data-Contest-2019-3.xlsx', sheet_name = 'Raw Data', skiprows=0)\n",
    "# print(df.head())\n",
    "df.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99956, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "32a0ff12-2826-41a6-9218-2b6b7f7f6aea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:39.415099Z",
     "start_time": "2024-12-10T23:10:39.367568Z"
    }
   },
   "source": [
    "# remove duplciated rows\n",
    "print(f\"Total number of duplicasted rows are: {sum([ele for ele in df.duplicated()])}\")\n",
    "duplicated_rows = df[df.duplicated(keep=False)] \n",
    "print(\"===\"*30)\n",
    "duplicates_summary = duplicated_rows.groupby(list(df.columns)).size().reset_index(name='Count')\n",
    "duplicates_summary = duplicates_summary[duplicates_summary['Count'] > 1]\n",
    "print(duplicates_summary)\n",
    "df_cleaned = df.drop_duplicates()\n",
    "print(\"===\"*30)\n",
    "print(f\"Number of rows before clean: {df.shape[0]}\")\n",
    "print(f\"Number of rows after clean : {df_cleaned.shape[0]}\")\n",
    "print(f\"Number of columns: {df_cleaned.shape[1]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of duplicasted rows are: 2840\n",
      "==========================================================================================\n",
      "     Order Date      Brand                           Sneaker Name  Sale Price  \\\n",
      "0    2017-10-13      Yeezy  Adidas-Yeezy-Boost-350-V2-Cream-White       425.0   \n",
      "1    2017-10-13      Yeezy        Adidas-Yeezy-Boost-350-V2-Zebra       560.0   \n",
      "2    2017-11-09  Off-White              Nike-Blazer-Mid-Off-White       550.0   \n",
      "3    2017-11-10      Yeezy  Adidas-Yeezy-Boost-350-V2-Cream-White       460.0   \n",
      "4    2017-11-11  Off-White              Nike-Air-Presto-Off-White       975.0   \n",
      "...         ...        ...                                    ...         ...   \n",
      "2413 2019-02-13      Yeezy       adidas-Yeezy-Boost-350-V2-Static       330.0   \n",
      "2414 2019-02-13      Yeezy       adidas-Yeezy-Boost-350-V2-Static       330.0   \n",
      "2415 2019-02-13      Yeezy       adidas-Yeezy-Boost-350-V2-Static       350.0   \n",
      "2416 2019-02-13  Off-White        Nike-Air-Max-90-Off-White-Black       565.0   \n",
      "2417 2019-02-13  Off-White   Nike-Air-Max-90-Off-White-Desert-Ore       520.0   \n",
      "\n",
      "      Retail Price Release Date  Shoe Size Buyer Region  Count  \n",
      "0              220   2017-04-29       10.5     Illinois      2  \n",
      "1              220   2017-02-25        8.5   California      2  \n",
      "2              130   2017-09-09        9.0   California      2  \n",
      "3              220   2017-04-29       10.0   California      2  \n",
      "4              160   2017-09-09       10.0   California      2  \n",
      "...            ...          ...        ...          ...    ...  \n",
      "2413           220   2018-12-27        9.0       Oregon      2  \n",
      "2414           220   2018-12-27       10.0       Oregon      2  \n",
      "2415           220   2018-12-27        9.5       Oregon      2  \n",
      "2416           160   2019-02-07        9.5       Oregon      2  \n",
      "2417           160   2019-02-07        9.0       Oregon      2  \n",
      "\n",
      "[2418 rows x 9 columns]\n",
      "==========================================================================================\n",
      "Number of rows before clean: 99956\n",
      "Number of rows after clean : 97116\n",
      "Number of columns: 8\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "c65c0142-54cc-4071-a329-28faaa5005a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:39.468379Z",
     "start_time": "2024-12-10T23:10:39.423556Z"
    }
   },
   "source": [
    "# X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "#     df_cleaned.drop(columns=['Sale Price']), df_cleaned['Sale Price'], test_size=0.2, random_state=42, stratify=df_cleaned['Brand']\n",
    "# )\n",
    "X_train_val, X_test, y_train_val, y_test = custom_train_test_split(\n",
    "    df_cleaned.drop(columns=['Sale Price']), df_cleaned['Sale Price'], test_size=0.2, random_state=42, stratify=df_cleaned['Brand']\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "7e53d7fc-0121-4443-af8a-8d63e3cefab4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:39.481051Z",
     "start_time": "2024-12-10T23:10:39.479473Z"
    }
   },
   "source": [
    "print(y_train_val.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77694,)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "c3c8b22b-e1a1-4056-9e62-e98b1ae0114f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:39.527608Z",
     "start_time": "2024-12-10T23:10:39.523374Z"
    }
   },
   "source": [
    "print(df_cleaned['Sale Price'].shape, type(df_cleaned['Sale Price']))\n",
    "print(df_cleaned.drop(columns=['Sale Price']).shape, type(df_cleaned.drop(columns=['Sale Price'])))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97116,) <class 'pandas.core.series.Series'>\n",
      "(97116, 7) <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "9f19d3cf-16c2-46e2-be46-1cdcf71da033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:39.571526Z",
     "start_time": "2024-12-10T23:10:39.570138Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "00dea487-b454-429a-accf-436603ffaa3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:39.646003Z",
     "start_time": "2024-12-10T23:10:39.614133Z"
    }
   },
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=X_train_val['Brand']\n",
    "# )\n",
    "X_train, X_val, y_train, y_val = custom_train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=X_train_val['Brand']\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "7861a410-c20e-4236-b28a-c5200d29b622",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:39.659940Z",
     "start_time": "2024-12-10T23:10:39.658775Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0ffee341-c4b5-4640-9fad-b12e7bc8c4b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:40.166490Z",
     "start_time": "2024-12-10T23:10:39.701305Z"
    }
   },
   "source": [
    "categorical_features = ['Order Date', 'Brand', 'Sneaker Name', 'Release Date', 'Buyer Region']\n",
    "\n",
    "# Initialize encoder\n",
    "# encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "encoder = CustomOneHotEncoder()\n",
    "\n",
    "# Fit and transform categorical features\n",
    "X_train_cat = encoder.fit_transform(X_train[categorical_features].to_numpy())\n",
    "X_val_cat = encoder.transform(X_val[categorical_features].to_numpy())\n",
    "X_test_cat = encoder.transform(X_test[categorical_features].to_numpy())"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "d41034ee-ed40-4eed-9baf-d7c91d9c2f42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:40.185111Z",
     "start_time": "2024-12-10T23:10:40.179988Z"
    }
   },
   "source": [
    "numerical_features = ['Retail Price', 'Shoe Size']\n",
    "\n",
    "# Initialize scaler\n",
    "# scaler = StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform numerical features\n",
    "X_train_num = scaler.fit_transform(X_train[numerical_features])\n",
    "X_val_num = scaler.transform(X_val[numerical_features])\n",
    "X_test_num = scaler.transform(X_test[numerical_features])"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "0789db85-d909-4b42-a2d5-d7adb0d03bd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:40.292863Z",
     "start_time": "2024-12-10T23:10:40.222021Z"
    }
   },
   "source": [
    "X_train_processed = np.hstack([X_train_num, X_train_cat])\n",
    "X_val_processed = np.hstack([X_val_num, X_val_cat])\n",
    "X_test_processed = np.hstack([X_test_num, X_test_cat])"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "e12e9ffa-5381-4fb9-874e-b452686a3e8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:40.335163Z",
     "start_time": "2024-12-10T23:10:40.308648Z"
    }
   },
   "source": [
    "X_train_tensor = torch.tensor(X_train_processed, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_processed, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32)\n",
    "\n",
    "# Convert targets\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "22e118c4-d9f5-4db8-90e0-e66561fa16af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:42.810103Z",
     "start_time": "2024-12-10T23:10:40.354024Z"
    }
   },
   "source": [
    "# save data\n",
    "import os\n",
    "data_folder_fp = \"data_folder\"\n",
    "os.makedirs(data_folder_fp, exist_ok=True)\n",
    "tensor_data_fp = os.path.join(data_folder_fp, \"pytorch_data.pt\")\n",
    "numpy_data_fp = os.path.join(data_folder_fp, \"numpy_data.pt\")\n",
    "torch.save((X_train_tensor, X_val_tensor, X_test_tensor, y_train_tensor, y_val_tensor, y_test_tensor), tensor_data_fp)\n",
    "torch.save((X_train_processed, X_val_processed, X_test_processed, y_train.to_numpy(), y_val.to_numpy(), y_test.to_numpy()), numpy_data_fp)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "d75da296-72fd-4482-88b0-dbd88187ab72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:42.831543Z",
     "start_time": "2024-12-10T23:10:42.829004Z"
    }
   },
   "source": [
    "print(y_train.size)\n",
    "y_train"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41977    260.0\n",
       "45598    480.0\n",
       "4201     439.0\n",
       "34155    350.0\n",
       "3470     241.0\n",
       "         ...  \n",
       "69806    451.0\n",
       "65122    821.0\n",
       "76747    695.0\n",
       "69422    707.0\n",
       "76792    587.0\n",
       "Name: Sale Price, Length: 58271, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "5042022d-e8b9-48d2-a226-33d58b202e71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:42.891012Z",
     "start_time": "2024-12-10T23:10:42.856715Z"
    }
   },
   "source": [
    "xtrain_data_fp = os.path.join(data_folder_fp, \"xtrain.pt\")\n",
    "torch.save(X_train, xtrain_data_fp)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "20580524-4679-4558-adda-a2e7539f8013",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:42.940221Z",
     "start_time": "2024-12-10T23:10:42.935932Z"
    }
   },
   "source": [
    "encoder_scaler_fp = os.path.join(data_folder_fp, \"encoder_scaler.pt\")\n",
    "torch.save((encoder, scaler), encoder_scaler_fp)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "b26e8681-5973-472b-b50f-88c412793192",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:10:43.059056Z",
     "start_time": "2024-12-10T23:10:43.057980Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
